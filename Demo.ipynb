{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCube Earth System Data Cube functionalities \n",
    "This notebook is a demonstration of some functionalities of the Earth System Data Cube technology  developed in the context of Horizon2020 project DeepCube and prepared for the final review meeting.\n",
    "\n",
    "The tools are developed in [Julia](https://julialang.org/), a high-performance and versatile scientific programming language. This Jupyter notebook must hence be run with a Julia kernel, preferably with Julia 1.10.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; Pkg.activate(\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running the notebook for the first time, after activating the environment in the cell above, the cell below needs to be run to instantiate the aforementioned environment in order to install all required packages and their dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages can then be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using YAXArrays, Zarr\n",
    "using OnlineStats: Mean, value, fit!, nobs\n",
    "using YAXArrays.Cubes: cubesize, formatbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we rely on [mesogeos](https://zenodo.org/records/7741518), a Mediterranean data cube for the modelling & analysis of wildfires developed in DeepCube. It is stored on the cloud and directly accessible as an s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = open_dataset(\"https://my-uc3-bucket.s3.gra.io.cloud.ovh.net/mesogeos.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show variables names\n",
    " print(keys(ds.cubes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 30 variables, which all have dimensions x (longitude) and y (latitude). Most of them also have a time dimension.\n",
    "\n",
    "We select one variable, `burned_areas`, to get information on the size of the cube and its chunking, that is, the way it is stored in compressed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dimensions\n",
    "println(\"Data cube size: $(size(ds.burned_areas))\")\n",
    "# get chunks\n",
    "println(\"Number of chunks: $(size(ds.burned_areas.chunks))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is chunked in a way most adapted for spatial analyses: each chunk contains data from one variable over its full spatial extent and only one time step.If we want to analyses more than one time step at a time, we have to load into memory as many chunks as time steps we want to analyse simultaneously. Even if our analysis focuses on a small spatial area, we have to load the full spatial extent into memory.\n",
    "\n",
    "Hence, for temporal analyses, it is more advisable to rechunk the dataset to chunks with a smaller spatial extent but with more time steps. The rechunking requires to first download the full dataset. It can then be done with YAXArrays.jl in two lines of code as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT RUN\n",
    "# dssub = ds[[:burned_areas,:net_ecosystem_exchange,:leaf_area_index]]\n",
    "\n",
    "# dschunked = setchunks(dssub,target_chunks)\n",
    "\n",
    "# savedataset(dschunked,path = \"/Net/Groups/BGI/work_3/scratch/fgans/DeepCube/UC3Cube_rechunked2.zarr\", max_cache=1e9, backend = :zarr,overwrite = false)\n",
    "# # max_cache determines the amount of memory to be used for rechunking, the larger this is, the faster the rechunking will go\n",
    "# # backend can be either :zarr or :netcdf\n",
    "# # setting overwrite=true will delete any existing dataset\n",
    "# # setting append=true will append the newly chunked variables to an existing data cube\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been rechunked to serve our purpose, we can access it and perform our temporal analysis.\n",
    "\n",
    "## Temporal analysis\n",
    "\n",
    "Our demo aims to analyse to which extent a selection of variables are associated with burned areas. Therefore, we compute the differences between a variable mean over time and over a spatial moving window `(7, 7)` inside burned areas and in their direct surroundings. Since most areas are never burned, we do not need to compute the means at every locations, but only where there was a fire. Hence, we first identify \"blobs\" of contiguous burned areas in these spatial windows and process them one at a time. That is, we load into memory just the chunks intersecting the spatial window and the duration of the fire event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the time series cube\n",
    "ds = open_dataset(\"/Net/Groups/BGI/work_3/scratch/fgans/DeepCube/UC3Cube_rechunked2.zarr\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select variables of interest\n",
    "burned_area = ds.burned_areas;\n",
    "preds = (\"lst_night\", \"lst_day\",\"dem\", \"lc_forest\", \"lc_grassland\", \"roads_distance\")\n",
    "possible_predictors = map(i->ds[Symbol(i)],preds);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total uncompressed data size:\n",
    "formatbytes(sum(cubesize,(burned_area,possible_predictors...)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indims_burnedarea = InDims(MovingWindow(\"x\",3,3), MovingWindow(\"y\",3,3), \"Time\", window_oob_value = 0.f0)\n",
    "indims_predictors = map(possible_predictors) do p\n",
    "    td = ndims(p) == 3 ? (\"Time\",) : ()\n",
    "    InDims(MovingWindow(\"x\",3,3), MovingWindow(\"y\",3,3), td..., window_oob_value = 0.f0)\n",
    "end\n",
    "\n",
    "outdims = OutDims(\n",
    "    Dim{:Variable}(collect(preds)), \n",
    "    outtype = Float32, \n",
    "    backend=:zarr,\n",
    "    path = \"./output.zarr\", \n",
    "    overwrite=true\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 20\n",
    "threads_per_worker = 16\n",
    "#Get 20 workers with 32 cpus per worker\n",
    "using ClusterManagers: SlurmManager\n",
    "using Distributed\n",
    "map(1:20) do i\n",
    "    Threads.@spawn begin\n",
    "        addprocs(\n",
    "            SlurmManager(1,fill(2.0,10)),\n",
    "            partition=\"big\",\n",
    "            mem_per_cpu=\"16GB\",\n",
    "            time=\"00:30:00\",\n",
    "            cpus_per_task=16,\n",
    "            exeflags=`--project=$(@__DIR__) -t 32 --heap-size-hint=8GB`\n",
    "            )\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load code everywhere\n",
    "@everywhere begin\n",
    "    using YAXArrays, Zarr\n",
    "    using OnlineStats: Mean, value, fit!, nobs\n",
    "    include(\"windowfire.jl\")\n",
    "    Zarr.Blosc.set_num_threads(16)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapCube(\n",
    "    fire_boundaries_window!, \n",
    "    (burned_area, possible_predictors...);\n",
    "    indims = (indims_burnedarea, indims_predictors...), \n",
    "    outdims = outdims,\n",
    "    max_cache=2e9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmprocs(workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrema(filter(i->!ismissing(i) && !isnan(i), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CairoMakie, Makie, GeoMakie\n",
    "using YAXArrays, Zarr\n",
    "ds = open_dataset(\"output.zarr/\")\n",
    "heatmap(data,colormap=:bluesreds)\n",
    "data  = reverse(ds.lst_day.data[:,:],dims=2)\n",
    "heatmap(data,colormap=:bluesreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
