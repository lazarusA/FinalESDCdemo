{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCube Earth System Data Cube functionalities \n",
    "This notebook is a demonstration of some functionalities of the Earth System Data Cube technology  developed in the context of Horizon2020 project DeepCube and prepared for the final review meeting.\n",
    "\n",
    "The tools are developed in [Julia](https://julialang.org/), a high-performance and versatile scientific programming language. This Jupyter notebook must hence be run with a Julia kernel, preferably with Julia 1.10.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the notebook for the first time, after activating the environment in the cell above, the cell below needs to be run to instantiate the aforementioned environment in order to install all required packages and their dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages can then be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using YAXArrays, Zarr\n",
    "using OnlineStats: Mean, value, fit!, nobs\n",
    "using YAXArrays.Cubes: cubesize, formatbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data cube\n",
    "\n",
    "For this demo, we rely on [mesogeos](https://zenodo.org/records/7741518), a Mediterranean data cube for the modelling & analysis of wildfires developed in DeepCube. It is stored on the cloud and directly accessible as an s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = open_dataset(\"https://my-uc3-bucket.s3.gra.io.cloud.ovh.net/mesogeos.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show variables names\n",
    " print(keys(ds.cubes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 30 variables, which all have dimensions x (longitude) and y (latitude). Most of them also have a time dimension.\n",
    "\n",
    "We select one variable, `burned_areas`, to get information on the size of the cube and its chunking, that is, the way it is stored in compressed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dimensions\n",
    "println(\"Data cube size: $(size(ds.burned_areas))\")\n",
    "# get chunks\n",
    "println(\"Number of chunks: $(size(ds.burned_areas.chunks))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is chunked in a way most adapted for spatial analyses: each chunk contains data from one variable over its full spatial extent and only one time step.If we want to analyses more than one time step at a time, we have to load into memory as many chunks as time steps we want to analyse simultaneously. Even if our analysis focuses on a small spatial area, we have to load the full spatial extent into memory.\n",
    "\n",
    "Hence, for temporal analyses, it is more advisable to rechunk the dataset to chunks with a smaller spatial extent but with more time steps. \n",
    "\n",
    "## Rechunking\n",
    "\n",
    "The rechunking requires to first download the full dataset. It can then be done with YAXArrays.jl in two lines of code as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT RUN\n",
    "# dssub = ds[[:burned_areas,:net_ecosystem_exchange,:leaf_area_index]]\n",
    "\n",
    "# dschunked = setchunks(dssub,target_chunks)\n",
    "\n",
    "# savedataset(dschunked,path = \"/Net/Groups/BGI/work_3/scratch/fgans/DeepCube/UC3Cube_rechunked2.zarr\", max_cache=1e9, backend = :zarr,overwrite = false)\n",
    "# # max_cache determines the amount of memory to be used for rechunking, the larger this is, the faster the rechunking will go\n",
    "# # backend can be either :zarr or :netcdf\n",
    "# # setting overwrite=true will delete any existing dataset\n",
    "# # setting append=true will append the newly chunked variables to an existing data cube\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been rechunked to serve our purpose, we can access it and perform our temporal analysis.\n",
    "\n",
    "## Temporal analysis\n",
    "\n",
    "Our demo aims to analyse to which extent the mean of a selection of variables are different inside and outside burnt areas. Therefore, we compute the difference between a variable mean over time and over a spatial moving window `(7, 7)` inside burnt areas and in their direct surroundings, that is, outside the burnt area but within the spatial window and over the duration of the fire event. Since most areas are never burnt, we do not need to compute the means at every location, but only where there was a fire. Hence, we first identify \"blobs\" of contiguous burnt areas in these spatial windows and process them one at a time. That is, we load in memory just the chunks intersecting the spatial window and the timesteps at which the area was burnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the time series cube\n",
    "ds = open_dataset(\"/Net/Groups/BGI/work_3/scratch/fgans/DeepCube/UC3Cube_rechunked2.zarr\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select six variables as potential predictors for our analysis of burnt areas: night and land surface temperature, elevation, forest and grass cover fractions and distance to nearest road.\n",
    "\n",
    "The total size of our uncompressed subdataset amounts to almost 1TB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables of interest\n",
    "burned_area = ds.burned_areas;\n",
    "preds = (\"lst_night\", \"lst_day\",\"dem\", \"lc_forest\", \"lc_grassland\", \"roads_distance\")\n",
    "possible_predictors = map(i->ds[Symbol(i)],preds);\n",
    "\n",
    "# Total uncompressed data size:\n",
    "formatbytes(sum(cubesize,(burned_area,possible_predictors...)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code lines, we define the dimensions over which we are going to run our analysis, that is, a `(7, 7, 6026)` moving window centred on our output grid cell, for our target and predictor variables (unless the latter does not have a time dimension). The output dimensions are defined as the predictor variables. Because we used `MovingWindow` in the input dimensions, the output will have `\"x\"` and `\"y\"` dimensions as the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indims_burnedarea = InDims(MovingWindow(\"x\",3,3), MovingWindow(\"y\",3,3), \"Time\", window_oob_value = 0.f0)\n",
    "indims_predictors = map(possible_predictors) do p\n",
    "    td = ndims(p) == 3 ? (\"Time\",) : ()\n",
    "    InDims(MovingWindow(\"x\",3,3), MovingWindow(\"y\",3,3), td..., window_oob_value = 0.f0)\n",
    "end\n",
    "\n",
    "outdims = OutDims(\n",
    "    Dim{:Variable}(collect(preds)), \n",
    "    outtype = Float32, \n",
    "    backend=:zarr,\n",
    "    path = \"./output.zarr\", \n",
    "    overwrite=true\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the job to be run on the cluster. The analysis is run on the MPI-BGC cluster but could be run on any cloud computing facility supporting Julia or docker containers.\n",
    "\n",
    "Therefore, we load two new Julia packages: `SlurmManager` to interact with the Slurm cluster and `Distributed`, which supports distributed computing. For some reason, on our partition for big data, the workers need to be spawned one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 20\n",
    "threads_per_worker = 16\n",
    "# Get 20 workers with 16 cpus per worker\n",
    "using ClusterManagers: SlurmManager\n",
    "using Distributed\n",
    "map(1:n_workers) do i\n",
    "    Threads.@spawn begin\n",
    "        addprocs(\n",
    "            SlurmManager(1,fill(2.0,10)),\n",
    "            partition=\"big\",\n",
    "            mem_per_cpu=\"16GB\",\n",
    "            time=\"00:30:00\",\n",
    "            cpus_per_task=threads_per_worker,\n",
    "            exeflags=`--project=$(@__DIR__) -t 32 --heap-size-hint=8GB`\n",
    "            )\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the workers are spawned, we load the required packages and functions on all of them. [windowfire.jl](windowfire.jl) is the script that contains the functions `fire_boundaries_window!` and `process_event`, which are at the core of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load code everywhere\n",
    "@everywhere begin\n",
    "    using YAXArrays, Zarr\n",
    "    using OnlineStats: Mean, value, fit!, nobs\n",
    "    include(\"windowfire.jl\")\n",
    "    Zarr.Blosc.set_num_threads(threads_per_worker)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fire_boundaries_window!` function is applied in parallel on the moving windows thanks to the `YAXArrays` function `mapCube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapCube(\n",
    "    fire_boundaries_window!, \n",
    "    (burned_area, possible_predictors...);\n",
    "    indims = (indims_burnedarea, indims_predictors...), \n",
    "    outdims = outdims,\n",
    "    max_cache=2e9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above code has been executed, the workers can be closed and the results can be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmprocs(workers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "The output data cube contains the differences between the averages inside and outside burnt areas in the direct surrounding (`(7, 7)` window) for the six possible predictor variables. It has no time dimension, only x and y.\n",
    "\n",
    "We can create maps of the output after loading plotting packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CairoMakie, Makie, GeoMakie, PerceptualColourMaps\n",
    "using YAXArrays, Zarr\n",
    "using Statistics\n",
    "\n",
    "if !isdir(\"./figs\")\n",
    "    mkdir(\"./figs\")\n",
    "end\n",
    "\n",
    "# output of moving windo demo\n",
    "ds = open_dataset(\"/Net/Groups/BGI/people/fgans/DeepCube/FinalESDCDEmo/output.zarr/\")\n",
    "\n",
    "preds = (\"lst_night\", \"lst_day\",\"dem\", \"lc_forest\", \"lc_grassland\", \"roads_distance\")\n",
    "possible_predictors = map(i->ds[Symbol(i)],preds);\n",
    "\n",
    "# units of the predictors, used in the legend of the maps\n",
    "units = (\"Temperature [Kelvin]\",\"Temperature [Kelvin]\",\"Elevation [m]\", \"Forest cover [%]\", \"Grass cover [%]\", \"Distance to nearest road [km]\")\n",
    "\n",
    "function myfig!(fig, lon, lat, data)\n",
    "    ax = GeoAxis(fig[1,1], limits=(extrema(lon), extrema(lat)))\n",
    "    s = surface!(ax, lon, lat, data; \n",
    "        colorrange=(-maximum(abs.(q)), maximum(abs.(q))),\n",
    "        colormap = cmap(\"CBD2\"),#:bluesreds, \n",
    "        # shading=`NoShading`,\n",
    "    )\n",
    "    cl=lines!(ax, GeoMakie.coastlines(), color = :black, linewidth=0.85)\n",
    "    # translate!(cl, 0, 0, 1000)\n",
    "    Colorbar(fig[1,2], s, label = units[i])\n",
    "    Label(fig[1, 1:end, Top()], \"Difference in average $(preds[i]) inside and outside burned areas\", fontsize=18, padding=(0, 6, 8, 0))\n",
    "\n",
    "    return(fig)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "    fig = Figure(;size=(1200,600));\n",
    "    c = possible_predictors[i]\n",
    "    lon = lookup(c, :x)\n",
    "    lat = lookup(c, :y)\n",
    "    data = c.data[:,:];\n",
    "\n",
    "    @show q = Statistics.quantile(filter(i->!ismissing(i) && !isnan(i), data),(0.1,0.5,0.9))\n",
    "\n",
    "    fig = myfig!(fig, lon, lat, data)\n",
    "    # save(\"./figs/$(preds[i]).png\", fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # subset cube \n",
    "    sc = c[x = 19 .. 26, y = 36 .. 42]\n",
    "    slon = lookup(sc, :x)\n",
    "    slat = lookup(sc, :y)\n",
    "    sdata = sc.data[:,:];\n",
    "    fig = Figure(;size=(500,500));\n",
    "    fig = myfig!(fig, slon, slat, sdata)\n",
    "    # save(\"./figs/$(preds[i])_zoom.png\", fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
